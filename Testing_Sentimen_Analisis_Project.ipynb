{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "!pip install nltk\n",
        "import pickle\n",
        "from pickle import UnpicklingError\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import requests\n",
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktKWIk2htW0U",
        "outputId": "4231cdaa-aed0-4548-b1cd-747c60e7866b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "wTZuSoojYrla"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9SjquDVVSNeH"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    with open('/content/svm_model.pkl', 'rb') as f:\n",
        "        svm = pickle.load(f)\n",
        "except (EOFError, UnpicklingError) as e:\n",
        "    print(f\"Error loading xgb model: {e}\")\n",
        "\n",
        "    print(\"Check if 'svm_model.pkl' exists and is not empty.\")\n",
        "    print(\"If the file was transferred, ensure it was done in binary mode.\")\n",
        "\n",
        "\n",
        "try:\n",
        "    with open('/content/tfidf_vectorizer.pkl', 'rb') as f:\n",
        "        tfidf = pickle.load(f)\n",
        "except (EOFError, UnpicklingError) as e:\n",
        "    print(f\"Error loading TF-IDF vectorizer: {e}\")\n",
        "\n",
        "\n",
        "def cleaningText(text):\n",
        "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
        "    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n",
        "    text = re.sub(r\"http\\S+\", '', text)\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = ' '.join([word for word in text.split() if word.lower() not in [\"mobile\", \"bni\", \"wondr\", \"bni mobile\"]])\n",
        "    text = text.strip(' ')\n",
        "    return text\n",
        "\n",
        "def casefoldingText(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def tokenizingText(text):\n",
        "    text = word_tokenize(text)\n",
        "    return text\n",
        "\n",
        "def filteringText(text):\n",
        "    listStopwords = set(stopwords.words('indonesian'))\n",
        "    listStopwords1 = set(stopwords.words('english'))\n",
        "    listStopwords.update(listStopwords1)\n",
        "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku','di','ya','loh','kah','deh'])\n",
        "    filtered = []\n",
        "    for txt in text:\n",
        "        if txt not in listStopwords:\n",
        "            filtered.append(txt)\n",
        "    text = filtered\n",
        "    return text\n",
        "\n",
        "def stemmingText(text):\n",
        "    factory = StemmerFactory()\n",
        "    stemmer = factory.create_stemmer()\n",
        "    words = text.split()\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "    return stemmed_text\n",
        "\n",
        "def toSentence(list_words):\n",
        "    sentence = ' '.join(word for word in list_words)\n",
        "    return sentence\n",
        "\n",
        "def fix_slangwords(text):\n",
        "    words = text.split()\n",
        "    fixed_words = []\n",
        "    for word in words:\n",
        "        if word.lower() in slangwords:\n",
        "            fixed_words.append(slangwords[word.lower()])\n",
        "        else:\n",
        "            fixed_words.append(word)\n",
        "    fixed_text = ' '.join(fixed_words)\n",
        "    return fixed_text\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/aninanandah/datasetproject/main/slangwords.json'  # URL tempat kamus slangwords disimpan\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    try:\n",
        "        slangwords = json.loads(response.text)\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error decoding JSON:\", e)\n",
        "        print(\"Response content:\", response.text)\n",
        "else:\n",
        "    print(\"Failed to fetch data from URL. Status code:\", response.status_code)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = cleaningText(text)\n",
        "    text = casefoldingText(text)\n",
        "    text = fix_slangwords(text)\n",
        "    text = tokenizingText(text)\n",
        "    text = filteringText(text)\n",
        "    text = toSentence(text)\n",
        "    return text\n",
        "\n",
        "def prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm):\n",
        "\n",
        "    review_baru_cleaned = cleaningText(review_baru)\n",
        "    review_baru_casefolded = casefoldingText(review_baru_cleaned)\n",
        "    review_baru_slangfixed = fix_slangwords(review_baru_casefolded)\n",
        "    review_baru_tokenized = tokenizingText(review_baru_slangfixed)\n",
        "    review_baru_filtered = filteringText(review_baru_tokenized)\n",
        "    review_baru_final = toSentence(review_baru_filtered)\n",
        "\n",
        "\n",
        "    X_review_baru = tfidf.transform([review_baru_final])\n",
        "\n",
        "\n",
        "    X_review_baru = X_review_baru.toarray()\n",
        "\n",
        "\n",
        "    prediksi_sentimen = svm.predict(X_review_baru)\n",
        "\n",
        "\n",
        "    if prediksi_sentimen[0] == 'positive':\n",
        "        hasil = \"Sentimen review baru adalah POSITIF.\"\n",
        "    elif prediksi_sentimen[0] == 'negative':\n",
        "        hasil = \"Sentimen review baru adalah NEGATIF.\"\n",
        "    else:\n",
        "        hasil = \"Sentimen review baru adalah NETRAL.\"\n",
        "\n",
        "    return hasil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \"sering ngelag\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "id": "3f20tvDfYyY9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bd8f3af9-d2b5-4eea-c0c8-e7bc11a17e1f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah NETRAL.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \" Sangat membantu, mudah\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "id": "tBGrqj2WYz4v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e335371d-d9d0-4f1e-b31e-11f8447c96db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah POSITIF.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_baru = \"sering eror\"\n",
        "prediksi_sentimen_kalimat_baru(review_baru, tfidf, svm)"
      ],
      "metadata": {
        "id": "XIfuYVYAY2J_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4d9677fa-4f58-4c9b-8e70-2231e8bc23e0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentimen review baru adalah NEGATIF.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}